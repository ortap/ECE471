{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_MNIST.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "WIURjGAcluvd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CNN on MNIST-10 Dataset\n",
        "\n",
        "### by Shailesh Patro\n",
        "\n",
        "This assignment was carried out with the inbuilt Tensorflow tutorial found here:  https://www.tensorflow.org/tutorials/estimators/cnn \n"
      ]
    },
    {
      "metadata": {
        "id": "e2-GSztNl5mH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.examples.tutorials.mnist import input_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3YBC8B2rpkUU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "e8e5a96a-4670-4856-b9ce-ff0be4627d34"
      },
      "cell_type": "code",
      "source": [
        "# Import MNIST data\n",
        "mnist = input_data.read_data_sets(\"MNIST-data/\", one_hot=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-ee2a2f009166>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aXSQRJSGp9sV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note to self: The one_hot option encodes the otherwise binary representation (3 = 0011 ) to a one_hot vector (3 = 1000)."
      ]
    },
    {
      "metadata": {
        "id": "0qSngKnOrcDc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Defining the architecture of my network:\n",
        "* (Input) -> [batch_size, 28, 28, 1]  >> Apply 32 filter of [5x5]\n",
        "* (Convolutional layer 1)  -> [batch_size, 28, 28, 32]\n",
        "* (ReLU 1)  -> [?, 28, 28, 32]\n",
        "* (Max pooling 1) -> [?, 14, 14, 32]\n",
        "* (Convolutional layer 2)  -> [?, 14, 14, 64] \n",
        "* (ReLU 2)  -> [?, 14, 14, 64] \n",
        "* (Max pooling 2)  -> [?, 7, 7, 64] \n",
        "* [fully connected layer 3] -> [1x1024]\n",
        "* [ReLU 3]  -> [1x1024]\n",
        "* [Drop out]  -> [1x1024]\n",
        "* [fully connected layer 4] -> [1x10]"
      ]
    },
    {
      "metadata": {
        "id": "ja3PataYpnNZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Initialize parameters for the model\n",
        "width = 28 # width of image in pixels\n",
        "height = 28  # height of the image in pixels\n",
        "n_inputs = width*height # number of pixels in one image/ number of inputs\n",
        "n_classes = 10 # There are 10 possible classifications for this problem"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fzoyR7IHr093",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Functions to format variables (kernel weights and biases) and to add to graph collections\n",
        "def format_weights(shape, name):\n",
        "  var = tf.get_variable(name = name, dtype = tf.float32, shape = shape, initializer = tf.contrib.layers.xavier_initializer_conv2d())\n",
        "  tf.add_to_collection('model_vars', var)\n",
        "  tf.add_to_collection('l2', tf.reduce_sum(tf.square(var)))\n",
        "  return var\n",
        "\n",
        "def format_biases(shape, name):\n",
        "  var = tf.get_variable(name = name, dtype = tf.float32, shape = shape, initializer = tf.constant_initializer(0.0))\n",
        "  tf.add_to_collection('model_vars', var)\n",
        "  tf.add_to_collection('l2', tf.reduce_sum(tf.square(var)))\n",
        "  return var"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oGigaIoRsauq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CNNmodel():\n",
        "  def __init__(self, sess, weights_dict, biases_dict, iterations, batch_size, learn_rate, reg_rate,  display_steps=100, n_inputs=n_inputs, n_classes=n_classes):\n",
        "    self.sess = sess\n",
        "\n",
        "    # previously defined parameters for the model\n",
        "    self.n_inputs = n_inputs\n",
        "    self.n_classes = n_classes\n",
        "\n",
        "    # get weights and biases in a dictionary for the various layers\n",
        "    self.weights_dict = weights_dict\n",
        "    self.biases_dict= biases_dict\n",
        "\n",
        "    # user-defined hyperparameters\n",
        "    self.iterations = iterations\n",
        "    self.batch_size = batch_size\n",
        "    self.display_steps = display_steps\n",
        "    self.learn_rate = learn_rate\n",
        "    self.reg_rate = reg_rate # is also called lambda?\n",
        "\n",
        "    # creae placeholders for inputs and outputs\n",
        "    self.x = tf.placeholder(tf.float32, [None, self.n_inputs])\n",
        "    self.y = tf.placeholder(tf.float32, [None, self.n_classes])\n",
        "    self.dropout = tf.placeholder(tf.float32)\n",
        "\n",
        "    self.cnn_model_fn()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AdtMjyl315Gv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Convole\n",
        "To create convoloution layer, I will use tf.nn.conv2d. The function takes in 4-D input and filter tensors, then computes a 2-D convolution.\n",
        "As inputs the function takes the following:\n",
        "\n",
        "*  x  in the shape - [batch_size, height, width, channels]\n",
        "*  weights in the shape - [filter_height, filter_width, input_channels, output_channels] (at least for the first layer filer/kernel is 5*5 is of shape [5*5*1,32])\n",
        "*  stride - how much the window shifts in each of the dimensions in the input tensor\n",
        "\n",
        "What happens through this function?\n",
        "*  Change the filter to a 2-D matrix with shape [5\\*5\\*1,32]\n",
        "*  Then it extracts image patches from the input tensor to form a *virtual* tensor of shape `[batch, 28, 28, 5*5*1]`.\n",
        "*  For each batch, right-multiplies the filter matrix and the image vector.\n",
        "\n",
        "The result is a `Tensor` (a 2-D convolution) of size <tf.Tensor 'add_7:0' shape=(?, 28, 28, 32)\n",
        "Here, the output of the first convolution layer is 32 [28x28] images. 32 is considered as volume/depth of the output image.\n",
        "### Adjust for bias\n",
        "The function defined below also utilized tf.nn.bias_add. Why is this useful? Well as the model learns it develops sources of bias. This tensorflow function can help correct that bias. It is very similar to tf.add.\n",
        "\n",
        "### Apply activation function\n",
        "All the outputs of the convolutional layer which is negative is replace by a 0 with the help of the ReLU activation function.\n",
        "\n",
        "### Apply max pooling\n",
        "Max Pooling is a method of non-linear down-sampling, by which the input image is partitioned into set of squares (or rectangles?) and then the maximum value in that region is obtained.\n"
      ]
    },
    {
      "metadata": {
        "id": "ERanMlJ41m6I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CNNmodel(CNNmodel):\n",
        "  def conv_layer(self, x, w, b, stride=1):\n",
        "    x = tf.nn.conv2d(x, w, strides=[1, stride, stride, 1], padding='SAME')\n",
        "    x = tf.nn.bias_add(x, b)\n",
        "    x = tf.nn.relu(x)\n",
        "    convlayer = tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "    return convlayer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mKVpGyp3SPXi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Creating the conv layers, fully connected layer, dropout and softmax layer"
      ]
    },
    {
      "metadata": {
        "id": "pDRESiQ7Kruv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CNNmodel(CNNmodel):\n",
        "  def cnn_model_fn(self):\n",
        "    \n",
        "    # Convert the images to tensors: input image is 28*28. The channel is 1\n",
        "    # referring to grayscale. the first number is batch size which can be any\n",
        "    # size so its denoted by -1\n",
        "    x_image = tf.reshape(self.x, [-1,28,28,1])\n",
        "    \n",
        "    # Convolutional layer 1\n",
        "    self.yhat = self.conv_layer(x_image, self.weights_dict['w1'], self.biases_dict['b1'])\n",
        "    # Convolutional layer 2\n",
        "    self.yhat = self.conv_layer(self.yhat, self.weights_dict['w2'], self.biases_dict['b2'])\n",
        "    \n",
        "    # Defining a fully connected layer to use softmax and to create the probabilities in the end.\n",
        "    \n",
        "    # Flatten second conv layer\n",
        "    self.yhat = tf.reshape(self.yhat, [-1, self.weights_dict['w3'].get_shape().as_list()[0]])\n",
        "    # Applying weights and biases\n",
        "    self.yhat = tf.add(tf.matmul(self.yhat, self.weights_dict['w3']), self.biases_dict['b3'])\n",
        "    # Apply the ReLU activation function\n",
        "    self.yhat = tf.nn.relu(self.yhat)\n",
        "    \n",
        "    # Create a dropout layer - an overfitting prevention tool\n",
        "    self.yhat = tf.nn.dropout(self.yhat, self.dropout)\n",
        "    \n",
        "    # Create a softmax layer\n",
        "    self.yhat = tf.add(tf.matmul(self.yhat, self.weights_dict['w4']), self.biases_dict['b4'])\n",
        "    # Define the cost function\n",
        "    self.costs = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.yhat, labels=self.y))\n",
        "    # Define the loss function with L2 regularization\n",
        "    self.l2 = tf.reduce_sum(tf.get_collection('l2'))\n",
        "    self.loss = self.costs + self.reg_rate * self.l2\n",
        "    \n",
        "    # Count the number cases in a mini-batch that has been classified correctly\n",
        "    self.correct_prediction = tf.equal(tf.argmax(self.yhat, 1), tf.argmax(self.y, 1))  \n",
        "    # Get accuracy using average of correct cases\n",
        "    self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wS0ZvbZfV3zA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CNNmodel(CNNmodel):\n",
        "  def train(self):\n",
        "    model_vars = tf.get_collection('model_vars')\n",
        "    self.optim = (tf.train.AdamOptimizer(learning_rate=self.learn_rate).minimize(self.loss, var_list=model_vars))\n",
        "    self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for it in range(self.iterations):\n",
        "      batch = mnist.train.next_batch(self.batch_size)\n",
        "      self.sess.run([self.optim], feed_dict={self.x: batch[0], self.y: batch[1], self.dropout: 0.75})\n",
        "      if it % self.display_steps == 0:\n",
        "        loss = self.sess.run(self.loss, feed_dict={self.x: batch[0], self.y: batch[1], self.dropout: 1.0})\n",
        "        train_accuracy = self.accuracy.eval(session=self.sess, feed_dict={self.x:batch[0], self.y: batch[1], self.dropout: 1.0})\n",
        "        print(\"Step: %d, Training accuracy: %g, Loss: %f\" % (it, float(train_accuracy), loss))\n",
        "      self.optim.run(session=self.sess,feed_dict={self.x: batch[0], self.y: batch[1], self.dropout: 0.5})\n",
        "    acc = self.sess.run(self.accuracy, feed_dict={self.x: mnist.validation.images[:5000], self.y: mnist.validation.labels[:5000], self.dropout: 1.0})\n",
        "    print(\"Validation Accuracy: \", acc)\n",
        "\n",
        "  def test_accuracy(self):\n",
        "    acc = self.sess.run(self.accuracy, feed_dict={self.x: mnist.test.images, self.y: mnist.test.labels, self.dropout: 1.0})\n",
        "    print(\"Test Accuracy: \", acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KeZyeUfAcRB0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "f6aa62a1-82ef-43b7-fa13-338a5e6ffe79"
      },
      "cell_type": "code",
      "source": [
        "sess_1 = tf.Session()\n",
        "\n",
        "weights_1 = {\n",
        "    'w1': format_weights([5, 5, 1, 32], 'w_1-1'),\n",
        "    'w2': format_weights([5, 5, 32, 64], 'w_1-2'),\n",
        "    'w3': format_weights([7 * 7 * 64, 1024], 'w_1-3'),\n",
        "    'w4': format_weights([1024, n_classes], 'w_1-4')\n",
        "}\n",
        "\n",
        "biases_1= {\n",
        "    'b1': format_biases([32], 'b_1-1'),\n",
        "    'b2': format_biases([64], 'b_1-2'),\n",
        "    'b3': format_biases([1024], 'b_1-3'),\n",
        "    'b4': format_biases([n_classes], 'b_1-4')\n",
        "}\n",
        "\n",
        "runs_1= 600\n",
        "batchsize_1 = 20\n",
        "learningrate_1 = 0.03\n",
        "regularizationrate_1 = 0.03\n",
        "\n",
        "\n",
        "model_1 = CNNmodel(sess_1, weights_1, biases_1, runs_1, batchsize_1, learningrate_1, regularizationrate_1)\n",
        "\n",
        "model_1.train()\n",
        "model_1.test_accuracy()\n",
        "\n",
        "sess_1.close()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step: 0, Training accuracy: 0.25, Loss: 45.495209\n",
            "Step: 100, Training accuracy: 0.55, Loss: 4.286979\n",
            "Step: 200, Training accuracy: 0.55, Loss: 6.666721\n",
            "Step: 300, Training accuracy: 0.6, Loss: 5.033250\n",
            "Step: 400, Training accuracy: 0.6, Loss: 3.635106\n",
            "Step: 500, Training accuracy: 0.55, Loss: 5.257320\n",
            "Validation Accuracy:  0.4788\n",
            "Test Accuracy:  0.4792\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tLB1Q6jmpC7_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "cc6c350b-dd61-49aa-914d-828a51c7a5f5"
      },
      "cell_type": "code",
      "source": [
        "sess_2 = tf.Session()\n",
        "\n",
        "weights_2 = {\n",
        "    'w1': format_weights([5, 5, 1, 32], 'w_2-1'),\n",
        "    'w2': format_weights([5, 5, 32, 64], 'w_2-2'),\n",
        "    'w3': format_weights([7 * 7 * 64, 1024], 'w_2-3'),\n",
        "    'w4': format_weights([1024, n_classes], 'w_2-4')\n",
        "}\n",
        "\n",
        "biases_2= {\n",
        "    'b1': format_biases([32], 'b_2-1'),\n",
        "    'b2': format_biases([64], 'b_2-2'),\n",
        "    'b3': format_biases([1024], 'b_2-3'),\n",
        "    'b4': format_biases([n_classes], 'b_2-4')\n",
        "}\n",
        "\n",
        "runs_2= 1000\n",
        "batchsize_2 = 20\n",
        "learningrate_2 = 0.003\n",
        "regularizationrate_2 = 0.003\n",
        "\n",
        "\n",
        "model_2 = CNNmodel(sess_2, weights_2, biases_2, runs_2, batchsize_2, learningrate_2, regularizationrate_2)\n",
        "\n",
        "model_2.train()\n",
        "model_2.test_accuracy()\n",
        "\n",
        "sess_2.close()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step: 0, Training accuracy: 0.2, Loss: 9.865908\n",
            "Step: 100, Training accuracy: 0.95, Loss: 0.729135\n",
            "Step: 200, Training accuracy: 0.95, Loss: 0.616921\n",
            "Step: 300, Training accuracy: 0.95, Loss: 0.600459\n",
            "Step: 400, Training accuracy: 0.9, Loss: 0.816150\n",
            "Step: 500, Training accuracy: 0.8, Loss: 0.951970\n",
            "Step: 600, Training accuracy: 0.95, Loss: 0.665529\n",
            "Step: 700, Training accuracy: 0.95, Loss: 0.437964\n",
            "Step: 800, Training accuracy: 0.95, Loss: 0.937742\n",
            "Step: 900, Training accuracy: 1, Loss: 0.396490\n",
            "Validation Accuracy:  0.9574\n",
            "Test Accuracy:  0.9576\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eGvPU3jDkopG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "outputId": "e91a6358-f269-4cd9-a7ce-7647de0ca9ce"
      },
      "cell_type": "code",
      "source": [
        "sess_3 = tf.Session()\n",
        "\n",
        "weights_3 = {\n",
        "    'w1': format_weights([5, 5, 1, 32], 'w_3-1'),\n",
        "    'w2': format_weights([5, 5, 32, 64], 'w_3-2'),\n",
        "    'w3': format_weights([7 * 7 * 64, 1024], 'w_3-3'),\n",
        "    'w4': format_weights([1024, n_classes], 'w_3-4')\n",
        "}\n",
        "\n",
        "biases_3= {\n",
        "    'b1': format_biases([32], 'b_3-1'),\n",
        "    'b2': format_biases([64], 'b_3-2'),\n",
        "    'b3': format_biases([1024], 'b_3-3'),\n",
        "    'b4': format_biases([n_classes], 'b_3-4')\n",
        "}\n",
        "\n",
        "runs_3 = 3000\n",
        "batchsize_3 = 20\n",
        "learningrate_3 = 0.003\n",
        "regularizationrate_3 = 0.0003\n",
        "\n",
        "\n",
        "model_3 = CNNmodel(sess_3, weights_3, biases_3, runs_3, batchsize_3, learningrate_3, regularizationrate_3)\n",
        "\n",
        "model_3.train()\n",
        "model_3.test_accuracy()\n",
        "sess_3.close()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step: 0, Training accuracy: 0.25, Loss: 2.816543\n",
            "Step: 100, Training accuracy: 1, Loss: 0.288369\n",
            "Step: 200, Training accuracy: 0.9, Loss: 0.612343\n",
            "Step: 300, Training accuracy: 0.9, Loss: 0.496815\n",
            "Step: 400, Training accuracy: 0.9, Loss: 0.568156\n",
            "Step: 500, Training accuracy: 0.95, Loss: 0.325526\n",
            "Step: 600, Training accuracy: 1, Loss: 0.308819\n",
            "Step: 700, Training accuracy: 0.95, Loss: 0.450259\n",
            "Step: 800, Training accuracy: 0.95, Loss: 0.356524\n",
            "Step: 900, Training accuracy: 1, Loss: 0.320757\n",
            "Step: 1000, Training accuracy: 1, Loss: 0.252354\n",
            "Step: 1100, Training accuracy: 1, Loss: 0.250033\n",
            "Step: 1200, Training accuracy: 1, Loss: 0.256277\n",
            "Step: 1300, Training accuracy: 0.95, Loss: 0.310451\n",
            "Step: 1400, Training accuracy: 1, Loss: 0.252973\n",
            "Step: 1500, Training accuracy: 1, Loss: 0.267030\n",
            "Step: 1600, Training accuracy: 1, Loss: 0.235063\n",
            "Step: 1700, Training accuracy: 1, Loss: 0.235208\n",
            "Step: 1800, Training accuracy: 1, Loss: 0.256866\n",
            "Step: 1900, Training accuracy: 0.95, Loss: 0.271403\n",
            "Step: 2000, Training accuracy: 1, Loss: 0.217616\n",
            "Step: 2100, Training accuracy: 0.95, Loss: 0.367333\n",
            "Step: 2200, Training accuracy: 1, Loss: 0.279232\n",
            "Step: 2300, Training accuracy: 0.95, Loss: 0.328460\n",
            "Step: 2400, Training accuracy: 1, Loss: 0.210814\n",
            "Step: 2500, Training accuracy: 1, Loss: 0.218794\n",
            "Step: 2600, Training accuracy: 0.95, Loss: 0.288470\n",
            "Step: 2700, Training accuracy: 1, Loss: 0.228684\n",
            "Step: 2800, Training accuracy: 1, Loss: 0.233607\n",
            "Step: 2900, Training accuracy: 1, Loss: 0.218819\n",
            "Validation Accuracy:  0.9748\n",
            "Test Accuracy:  0.9702\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x5C6IMi5p9Cz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "outputId": "9fedc52e-d261-4e12-93ce-b1112746af78"
      },
      "cell_type": "code",
      "source": [
        "sess_5 = tf.Session()\n",
        "\n",
        "weights_5 = {\n",
        "    'w1': format_weights([5, 5, 1, 32], 'w_5-1'),\n",
        "    'w2': format_weights([5, 5, 32, 64], 'w_5-2'),\n",
        "    'w3': format_weights([7 * 7 * 64, 1024], 'w_5-3'),\n",
        "    'w4': format_weights([1024, n_classes], 'w_5-4')\n",
        "}\n",
        "\n",
        "biases_5= {\n",
        "    'b1': format_biases([32], 'b_5-1'),\n",
        "    'b2': format_biases([64], 'b_5-2'),\n",
        "    'b3': format_biases([1024], 'b_5-3'),\n",
        "    'b4': format_biases([n_classes], 'b_5-4')\n",
        "}\n",
        "\n",
        "runs_5 = 3000\n",
        "batchsize_5 = 50\n",
        "learningrate_5 = 0.001\n",
        "regularizationrate_5 = 0.0001\n",
        "\n",
        "\n",
        "model_5 = CNNmodel(sess_5, weights_5, biases_5, runs_5, batchsize_5, learningrate_5, regularizationrate_5)\n",
        "\n",
        "model_5.train()\n",
        "model_5.test_accuracy()\n",
        "sess_5.close()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step: 0, Training accuracy: 0.22, Loss: 2.667605\n",
            "Step: 100, Training accuracy: 0.92, Loss: 0.257868\n",
            "Step: 200, Training accuracy: 0.98, Loss: 0.169784\n",
            "Step: 300, Training accuracy: 1, Loss: 0.096193\n",
            "Step: 400, Training accuracy: 1, Loss: 0.107289\n",
            "Step: 500, Training accuracy: 0.98, Loss: 0.163559\n",
            "Step: 600, Training accuracy: 1, Loss: 0.093809\n",
            "Step: 700, Training accuracy: 1, Loss: 0.092286\n",
            "Step: 800, Training accuracy: 0.98, Loss: 0.106344\n",
            "Step: 900, Training accuracy: 0.96, Loss: 0.217415\n",
            "Step: 1000, Training accuracy: 0.98, Loss: 0.116735\n",
            "Step: 1100, Training accuracy: 1, Loss: 0.104654\n",
            "Step: 1200, Training accuracy: 0.94, Loss: 0.271424\n",
            "Step: 1300, Training accuracy: 0.98, Loss: 0.115138\n",
            "Step: 1400, Training accuracy: 0.96, Loss: 0.265512\n",
            "Step: 1500, Training accuracy: 1, Loss: 0.082118\n",
            "Step: 1600, Training accuracy: 0.96, Loss: 0.252909\n",
            "Step: 1700, Training accuracy: 1, Loss: 0.076609\n",
            "Step: 1800, Training accuracy: 1, Loss: 0.083297\n",
            "Step: 1900, Training accuracy: 1, Loss: 0.084265\n",
            "Step: 2000, Training accuracy: 1, Loss: 0.088757\n",
            "Step: 2100, Training accuracy: 1, Loss: 0.085878\n",
            "Step: 2200, Training accuracy: 1, Loss: 0.082845\n",
            "Step: 2300, Training accuracy: 1, Loss: 0.075932\n",
            "Step: 2400, Training accuracy: 1, Loss: 0.097019\n",
            "Step: 2500, Training accuracy: 0.98, Loss: 0.124391\n",
            "Step: 2600, Training accuracy: 1, Loss: 0.080097\n",
            "Step: 2700, Training accuracy: 1, Loss: 0.078600\n",
            "Step: 2800, Training accuracy: 0.98, Loss: 0.107334\n",
            "Step: 2900, Training accuracy: 0.98, Loss: 0.107224\n",
            "Validation Accuracy:  0.9854\n",
            "Test Accuracy:  0.9866\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}