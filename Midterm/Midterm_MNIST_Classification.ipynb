{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Midterm - MNIST Classification.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "sHL90MU0YgWP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Midterm - Adam with MNIST Classification\n",
        "### Shailesh Patro\n",
        "\n",
        "I have implemented an Adam optimizer based on *ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION* by Diederik P. Kingma & Jimmy Lei Ba         \n",
        "(https://arxiv.org/pdf/1412.6980v8.pdf). \n",
        "The implementation uses Keras base class and common methods for compatbility with a keras model. The keras compatibility is preferable for testing the optimizer quickly on several datasets.\n",
        "\n",
        "I have tested the optimizer on three data sets:\n",
        "\n",
        "1.   Sine Curve Regression\n",
        "2.   MNIST Classification\n",
        "3.   AG News data Classification\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "230Bg_DxWTgG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "\n",
        "from keras.optimizers import Optimizer\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Ope2Np1WWIa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "793ad859-c2aa-4193-fce6-3ee6c4a3bbd9"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 4\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6bc_ERDlWdb1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class AdamOptimizer(Optimizer):\n",
        "  def __init__(self, alpha=0.001, beta_1=0.9,\n",
        "               beta_2=0.999, epsilon=1e-08, \n",
        "               **kwargs):\n",
        "    super(AdamOptimizer, self).__init__(**kwargs)\n",
        "    with keras.backend.name_scope(self.__class__.__name__):\n",
        "      self.iterations = keras.backend.variable(0, dtype='int64', name='iterations')\n",
        "      # alpha is the stepsize/learning rate as described in the paper\n",
        "      self.alpha = keras.backend.variable(alpha, name='alpha')\n",
        "      # beta_1, beta_2 are the exponential decay rates for the moment estimates\n",
        "      self.beta_1 = keras.backend.variable(beta_1, name='beta_1')\n",
        "      self.beta_2 = keras.backend.variable(beta_2, name='beta_2')\n",
        "      self.epsilon = epsilon\n",
        " \n",
        "\n",
        "\n",
        "  def get_updates(self, loss, params):\n",
        "    xs = params\n",
        "    # get gradients with tensorflow's built in gradient function\n",
        "    grads = tf.gradients(loss, xs, colocate_gradients_with_ops=True)\n",
        "    self.updates = [tf.assign_add(self.iterations, 1)]\n",
        "    # alpha is the learning rate as defined in the paper\n",
        "    alpha = self.alpha\n",
        "    # increment timestep by 1\n",
        "    t = tf.cast(self.iterations, 'float32') + 1\n",
        "    \n",
        "    # suggested improvement as mentioned in section 2: algorithm\n",
        "    alpha_t = alpha * (tf.sqrt(1. - tf.pow(self.beta_2, t)) / (1. - tf.pow(self.beta_1, t))) \n",
        "    \n",
        "    # initialize m, v to zero\n",
        "    ms = [keras.backend.zeros(x.shape, dtype=x.dtype.base_dtype.name) for x in xs]\n",
        "    vs = [keras.backend.zeros(x.shape, dtype=x.dtype.base_dtype.name) for x in xs]\n",
        " \n",
        "    self.weights = [self.iterations] + ms + vs\n",
        "    \n",
        "    for x, g, m, v in zip(xs, grads, ms, vs):\n",
        "        # Update biased first moment estimate\n",
        "        m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
        "        \n",
        "        # Update biased second raw moment estimate \n",
        "        # also used tensorflow's elementwise square\n",
        "        v_t = (self.beta_2 * v) + (1. - self.beta_2) * tf.square(g) \n",
        "        \n",
        "        # Update Parameters\n",
        "        x_t = x - alpha_t * m_t / (tf.sqrt(v_t) + self.epsilon)\n",
        "        self.updates.append(tf.assign(m, m_t))\n",
        "        self.updates.append(tf.assign(v, v_t))\n",
        "        new_x = x_t\n",
        "\n",
        "        self.updates.append(tf.assign(x, new_x))\n",
        "    return self.updates\n",
        "\n",
        "  \n",
        "  \n",
        "  def get_config(self):\n",
        "    config = {'alpha': float(keras.backend.get_value(self.alpha)),\n",
        "              'beta_1': float(keras.backend.get_value(self.beta_1)),\n",
        "              'beta_2': float(keras.backend.get_value(self.beta_2)),\n",
        "              'epsilon': self.epsilon}\n",
        "    base_config = super(AdamOptimizer, self).get_config()\n",
        "    return dict(list(base_config.items()) + list(config.items()))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6b_PLNHcWdvp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2845d31d-49a1-4d7e-935b-7098c25d0626"
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "adamopt = AdamOptimizer()\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=adamopt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/4\n",
            "60000/60000 [==============================] - 164s 3ms/step - loss: 0.2434 - acc: 0.9269 - val_loss: 0.0571 - val_acc: 0.9814\n",
            "Epoch 2/4\n",
            "60000/60000 [==============================] - 167s 3ms/step - loss: 0.0851 - acc: 0.9741 - val_loss: 0.0389 - val_acc: 0.9860\n",
            "Epoch 3/4\n",
            "60000/60000 [==============================] - 166s 3ms/step - loss: 0.0637 - acc: 0.9812 - val_loss: 0.0389 - val_acc: 0.9865\n",
            "Epoch 4/4\n",
            "60000/60000 [==============================] - 166s 3ms/step - loss: 0.0530 - acc: 0.9841 - val_loss: 0.0353 - val_acc: 0.9887\n",
            "Test loss: 0.035345532561086294\n",
            "Test accuracy: 0.9887\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TrMkBswSXDrv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}